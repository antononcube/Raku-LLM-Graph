{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Summarize large text\n",
        "\n",
        "<p style=\"font-size: 20px; font-weight: bold; font-style: italic;\">...via LangGraph</p>\n",
        "\n",
        "Anton Antonov   \n",
        "February 2026\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Introduction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This notebook illustrates how to specify a Large Language Model (LLM) graph for deriving comprehensive summaries of large texts.\n",
        "The LLM graph is based on different LLM and non-LLM functions.\n",
        "\n",
        "The Python package [LangGraph](https://github.com/langchain-ai/langgraph) is used.\n",
        "A similar implementation based on the Raku package [\"LLM::Graph\"](https://raku.land/zef:antononcube/LLM::Graph) is given in the notebook\n",
        "[\"Summarize-large-text\"](Summarize-large-text.ipynb).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Setup\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load the LangChain packages:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import TypedDict, Optional, Dict, Any, List\n",
        "\n",
        "import json\n",
        "import re\n",
        "from pathlib import Path\n",
        "\n",
        "import requests\n",
        "\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langchain_core.messages import HumanMessage\n",
        "from langchain_ollama import ChatOllama\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load Markdown display package:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from IPython.display import HTML, Markdown, display\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "LLM access configurations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Make sure Ollama is running and the model is available.\n",
        "llm_ollama = ChatOllama(model=\"gpt-oss:20b\", temperature=0.5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "llm = llm_ollama\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## LLM graph\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Nodes spec:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SummaryState(TypedDict, total=False):\n",
        "    input: str\n",
        "    with_title: Optional[str]\n",
        "    export_and_open: Optional[str]\n",
        "    input_type: str\n",
        "    ingest_text: str\n",
        "    title: str\n",
        "    summary: str\n",
        "    topics_table: str\n",
        "    thinking_hats: str\n",
        "    mind_map: str\n",
        "    most_provocative: str\n",
        "    report: str\n",
        "\n",
        "\n",
        "def _invoke(prompt: str) -> str:\n",
        "    msg = llm.invoke([HumanMessage(content=prompt)])\n",
        "    return getattr(msg, \"content\", str(msg))\n",
        "\n",
        "\n",
        "def _truthy(value: Any) -> bool:\n",
        "    if isinstance(value, bool):\n",
        "        return value\n",
        "    if value is None:\n",
        "        return False\n",
        "    return str(value).strip().lower() in {\"true\", \"yes\", \"1\", \"y\", \"open\"}\n",
        "\n",
        "\n",
        "def _strip_fences(text: str) -> str:\n",
        "    text = text.strip()\n",
        "    text = re.sub(r\"^```[a-zA-Z0-9_-]*\", \"\", text)\n",
        "    text = re.sub(r\"```$\", \"\", text)\n",
        "    return text.strip()\n",
        "\n",
        "\n",
        "def _extract_json(text: str) -> List[Dict[str, Any]]:\n",
        "    cleaned = _strip_fences(text)\n",
        "    try:\n",
        "        data = json.loads(cleaned)\n",
        "        if isinstance(data, list):\n",
        "            return data\n",
        "    except Exception:\n",
        "        pass\n",
        "    match = re.search(r\"(\\[.*\\])\", cleaned, re.S)\n",
        "    if match:\n",
        "        try:\n",
        "            data = json.loads(match.group(1))\n",
        "            if isinstance(data, list):\n",
        "                return data\n",
        "        except Exception:\n",
        "            return []\n",
        "    return []\n",
        "\n",
        "\n",
        "def _table_to_html(rows: List[Dict[str, Any]], columns: Optional[List[str]] = None) -> str:\n",
        "    if not rows:\n",
        "        return \"<p><em>No data.</em></p>\"\n",
        "    if columns is None:\n",
        "        columns = sorted({key for row in rows for key in row.keys()})\n",
        "    header = \"\".join(f\"<th>{col}</th>\" for col in columns)\n",
        "    body_rows = []\n",
        "    for row in rows:\n",
        "        body_rows.append(\"<tr>\" + \"\".join(f\"<td>{row.get(col, '')}</td>\" for col in columns) + \"</tr>\")\n",
        "    body = \"\".join(body_rows)\n",
        "    return f\"<table><thead><tr>{header}</tr></thead><tbody>{body}</tbody></table>\"\n",
        "\n",
        "\n",
        "def _guess_input_type(text: str) -> str:\n",
        "    if not isinstance(text, str):\n",
        "        return \"Other\"\n",
        "    stripped = text.strip()\n",
        "    if re.match(r\"^https?://\", stripped):\n",
        "        return \"URL\"\n",
        "    if Path(stripped).exists():\n",
        "        return \"FilePath\"\n",
        "    if stripped:\n",
        "        return \"Text\"\n",
        "    return \"Other\"\n",
        "\n",
        "\n",
        "def type_of_input(state: SummaryState) -> Dict[str, Any]:\n",
        "    if state.get(\"input_type\"):\n",
        "        return {}\n",
        "    raw = state.get(\"input\", \"\")\n",
        "    guessed = _guess_input_type(raw)\n",
        "    if guessed in {\"URL\", \"FilePath\"}:\n",
        "        return {\"input_type\": guessed}\n",
        "    prompt = (\n",
        "        \"Determine the input type of:\\n\",\n",
        "        f\"{raw}\\n\\n\",\n",
        "        \"The result should be one of: 'Text', 'URL', 'FilePath', or 'Other'.\"\n",
        "    )\n",
        "    result = _invoke(prompt).strip()\n",
        "    result = result.strip(\"' \")\n",
        "    if result not in {\"Text\", \"URL\", \"FilePath\", \"Other\"}:\n",
        "        result = guessed\n",
        "    return {\"input_type\": result}\n",
        "\n",
        "\n",
        "def ingest_text(state: SummaryState) -> Dict[str, Any]:\n",
        "    if state.get(\"ingest_text\"):\n",
        "        return {}\n",
        "    raw = state.get(\"input\", \"\")\n",
        "    input_type = state.get(\"input_type\", \"Other\")\n",
        "    if input_type == \"URL\":\n",
        "        response = requests.get(raw, timeout=30)\n",
        "        response.raise_for_status()\n",
        "        return {\"ingest_text\": response.text}\n",
        "    if input_type == \"FilePath\":\n",
        "        return {\"ingest_text\": Path(raw).read_text()}\n",
        "    return {\"ingest_text\": raw}\n",
        "\n",
        "\n",
        "def title_node(state: SummaryState) -> Dict[str, Any]:\n",
        "    if state.get(\"title\"):\n",
        "        return {}\n",
        "    with_title = state.get(\"with_title\")\n",
        "    if isinstance(with_title, str) and with_title.strip():\n",
        "        return {\"title\": with_title.strip()}\n",
        "    prompt = (\n",
        "        \"Suggest a short title (6 words or fewer) for the following article:\\n\\n\"\n",
        "        f\"{state.get('ingest_text', '')}\\n\\n\"\n",
        "        \"Return only the title.\"\n",
        "    )\n",
        "    return {\"title\": _invoke(prompt).strip()}\n",
        "\n",
        "\n",
        "def summary_node(state: SummaryState) -> Dict[str, Any]:\n",
        "    if state.get(\"summary\"):\n",
        "        return {}\n",
        "    prompt = (\n",
        "        \"Summarize the following text in a comprehensive but concise way:\\n\\n\"\n",
        "        f\"{state.get('ingest_text', '')}\"\n",
        "    )\n",
        "    return {\"summary\": _invoke(prompt).strip()}\n",
        "\n",
        "\n",
        "def topics_table_node(state: SummaryState) -> Dict[str, Any]:\n",
        "    if state.get(\"topics_table\"):\n",
        "        return {}\n",
        "    prompt = (\n",
        "        \"Create a JSON array of objects with fields 'theme' and 'content' summarizing up to 20 key topics in this article:\\n\\n\"\n",
        "        f\"{state.get('ingest_text', '')}\\n\\n\"\n",
        "        \"Return JSON only.\"\n",
        "    )\n",
        "    return {\"topics_table\": _invoke(prompt).strip()}\n",
        "\n",
        "\n",
        "def thinking_hats_node(state: SummaryState) -> Dict[str, Any]:\n",
        "    if state.get(\"thinking_hats\"):\n",
        "        return {}\n",
        "    prompt = (\n",
        "        \"Provide De Bono thinking hats feedback using ONLY yellow and grey hats.\\n\\n\"\n",
        "        \"Return HTML only.\\n\\n\"\n",
        "        f\"{state.get('ingest_text', '')}\"\n",
        "    )\n",
        "    return {\"thinking_hats\": _invoke(prompt).strip()}\n",
        "\n",
        "\n",
        "def mind_map_node(state: SummaryState) -> Dict[str, Any]:\n",
        "    if state.get(\"mind_map\"):\n",
        "        return {}\n",
        "    prompt = (\n",
        "        \"Create a Mermaid mind map or flowchart that captures the main ideas of the text.\\n\\n\"\n",
        "        \"Return only Mermaid code.\\n\\n\"\n",
        "        f\"{state.get('ingest_text', '')}\"\n",
        "    )\n",
        "    mermaid = _invoke(prompt).strip()\n",
        "    return {\"mind_map\": mermaid}\n",
        "\n",
        "\n",
        "def most_provocative_node(state: SummaryState) -> Dict[str, Any]:\n",
        "    if state.get(\"most_provocative\"):\n",
        "        return {}\n",
        "    prompt = (\n",
        "        \"Give a JSON array of the most important or provocative statements in the following text.\\n\\n\"\n",
        "        \"Each item should have fields 'statement' and 'notes'.\\n\\n\"\n",
        "        f\"{state.get('ingest_text', '')}\\n\\n\"\n",
        "        \"Return JSON only.\"\n",
        "    )\n",
        "    return {\"most_provocative\": _invoke(prompt).strip()}\n",
        "\n",
        "\n",
        "def report_node(state: SummaryState) -> Dict[str, Any]:\n",
        "    if state.get(\"report\"):\n",
        "        return {}\n",
        "    topics = _extract_json(state.get(\"topics_table\", \"\"))\n",
        "    provocative = _extract_json(state.get(\"most_provocative\", \"\"))\n",
        "    topics_html = _table_to_html(topics, columns=[\"theme\", \"content\"])\n",
        "    prov_html = _table_to_html(provocative, columns=[\"statement\", \"notes\"])\n",
        "    thinking_hats = _strip_fences(state.get(\"thinking_hats\", \"\"))\n",
        "    mind_map = state.get(\"mind_map\", \"\")\n",
        "\n",
        "    if not mind_map.strip().startswith(\"```\"):\n",
        "\n",
        "        mind_map = f\"```mermaid\\n{mind_map}\\n```\"\n",
        "\n",
        "    report = \"\\n\\n\".join(\n",
        "        [\n",
        "            f\"# {state.get('title', '')}\",\n",
        "            \"### *LLM summary report*\",\n",
        "            \"## Summary\",\n",
        "            state.get(\"summary\", \"\"),\n",
        "            \"## Topics\",\n",
        "            topics_html,\n",
        "            \"## Mind map\",\n",
        "            mind_map,\n",
        "            \"## Thinking hats\",\n",
        "            thinking_hats,\n",
        "            \"## Most important or provocative statements\",\n",
        "            prov_html,\n",
        "        ])\n",
        "    return {\"report\": report}\n",
        "\n",
        "\n",
        "def export_and_open_node(state: SummaryState) -> Dict[str, Any]:\n",
        "    if not _truthy(state.get(\"export_and_open\")):\n",
        "        return {}\n",
        "    report = state.get(\"report\", \"\")\n",
        "    Path(\"Report.md\").write_text(report)\n",
        "    return {\"export_and_open\": \"Report.md\"}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Make the graph:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "graph = StateGraph(SummaryState)\n",
        "\n",
        "graph.add_node(\"TypeOfInput\", type_of_input)\n",
        "graph.add_node(\"IngestText\", ingest_text)\n",
        "graph.add_node(\"Title\", title_node)\n",
        "graph.add_node(\"Summary\", summary_node)\n",
        "graph.add_node(\"TopicsTable\", topics_table_node)\n",
        "graph.add_node(\"ThinkingHats\", thinking_hats_node)\n",
        "graph.add_node(\"MindMap\", mind_map_node)\n",
        "graph.add_node(\"MostProvocative\", most_provocative_node)\n",
        "graph.add_node(\"Report\", report_node)\n",
        "graph.add_node(\"ExportAndOpen\", export_and_open_node)\n",
        "\n",
        "graph.set_entry_point(\"TypeOfInput\")\n",
        "\n",
        "graph.add_edge(\"TypeOfInput\", \"IngestText\")\n",
        "graph.add_edge(\"IngestText\", \"Title\")\n",
        "graph.add_edge(\"IngestText\", \"Summary\")\n",
        "graph.add_edge(\"IngestText\", \"TopicsTable\")\n",
        "graph.add_edge(\"IngestText\", \"ThinkingHats\")\n",
        "graph.add_edge(\"IngestText\", \"MindMap\")\n",
        "graph.add_edge(\"IngestText\", \"MostProvocative\")\n",
        "graph.add_edge(\"Title\", \"Report\")\n",
        "graph.add_edge(\"Summary\", \"Report\")\n",
        "graph.add_edge(\"TopicsTable\", \"Report\")\n",
        "graph.add_edge(\"ThinkingHats\", \"Report\")\n",
        "graph.add_edge(\"MindMap\", \"Report\")\n",
        "graph.add_edge(\"MostProvocative\", \"Report\")\n",
        "\n",
        "def _should_export(state: SummaryState) -> str:\n",
        "    return \"export\" if _truthy(state.get(\"export_and_open\")) else \"end\"\n",
        "\n",
        "graph.add_conditional_edges(\"Report\", _should_export, {\"export\": \"ExportAndOpen\", \"end\": END})\n",
        "graph.add_edge(\"ExportAndOpen\", END)\n",
        "\n",
        "app = graph.compile()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb177433",
      "metadata": {},
      "outputs": [],
      "source": [
        "app"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Full computation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "URL and text statistics:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def text_stats(text: str) -> Dict[str, int]:\n",
        "    words = re.findall(r\"\\w+\", text)\n",
        "    return {\n",
        "        \"characters\": len(text),\n",
        "        \"words\": len(words),\n",
        "        \"lines\": len(text.splitlines()),\n",
        "    }\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/antononcube/RakuForPrediction-blog/refs/heads/main/Data/Graph-neat-examples-in-Raku-Set-2-YouTube.txt\"\n",
        "txt_focus = requests.get(url, timeout=30).text\n",
        "\n",
        "text_stats(txt_focus)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Computation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "result = app.invoke({\"input\": url, \"with_title\": \"«Graph» neat examples, set 3\"})\n",
        "# result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Show the corresponding graph-plot:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "app"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To get the actual Mermaid code use of the graph plot: \n",
        "\n",
        "```python\n",
        "from IPython.display import Markdown, display\n",
        "display(Markdown(\"```mermaid\\n\" + app.get_graph().draw_mermaid() + \"\\n```\"))\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Final result:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "display(Markdown(result.get(\"report\", \"\")))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Partial evaluation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here all results are pre-assigned as arguments:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "partial = app.invoke(\n",
        "    {\n",
        "        \"input\": url,\n",
        "        \"with_title\": \"«Graph» neat examples, set 3\",\n",
        "        \"export_and_open\": \"yes\",\n",
        "        \"input_type\": \"Other\",\n",
        "        \"summary\": \"In brief\",\n",
        "        \"ingest_text\": \"Ingest text\",\n",
        "        \"topics_table\": '[{\"theme\": \"TopicsTable\", \"content\": \"...\"}]',\n",
        "        \"thinking_hats\": \"<p>Thinking hats</p>\",\n",
        "        \"mind_map\": \"mind map graph\",\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0bc0a243",
      "metadata": {},
      "outputs": [],
      "source": [
        "display(Markdown(partial.get(\"report\", \"\")))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## References\n",
        "\n",
        "### Blog posts\n",
        "\n",
        "[AA1] Anton Antonov,\n",
        "[\"Parameterized Literate Programming\"](https://rakuforprediction.wordpress.com/2025/06/21/parameterized-literate-programming/),\n",
        "(2025),\n",
        "[RakuForPrediction at WordPress](https://rakuforprediction.wordpress.com).\n",
        "\n",
        "### Notebooks\n",
        "\n",
        "[AAn1] Anton Antonov,\n",
        "[\"LLM comprehensive summary template for large texts\"](https://community.wolfram.com/groups/-/m/t/3448842),\n",
        "(2025)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "SciPyCentric",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
